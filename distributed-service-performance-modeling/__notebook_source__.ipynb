import numpy as np
import pandas as pd
import matplotlib.pyplot as plt



def simulate_mm1(arrival_rate, service_rate, num_requests=10000):
    """
    Simulate M/M/1 queue:
    arrival_rate = lambda (requests/sec)
    service_rate = mu (requests/sec)
    """
    arrival_times = np.cumsum(np.random.exponential(1/arrival_rate, num_requests))
    service_times = np.random.exponential(1/service_rate, num_requests)

    start_service = np.zeros(num_requests)
    finish_service = np.zeros(num_requests)

    for i in range(num_requests):
        if i == 0:
            start_service[i] = arrival_times[i]
        else:
            start_service[i] = max(arrival_times[i], finish_service[i-1])

        finish_service[i] = start_service[i] + service_times[i]

    latency = finish_service - arrival_times
    return latency



arrival_rate = 50  # incoming requests/sec
service_rates = [60, 70, 80, 100]  # different server capacities

results = []

for mu in service_rates:
    latency = simulate_mm1(arrival_rate, mu)

    results.append({
        "Service Rate": mu,
        "Utilization (ρ)": arrival_rate/mu,
        "Avg Latency": np.mean(latency),
        "P95 Latency": np.percentile(latency,95),
        "P99 Latency": np.percentile(latency,99)
    })

df = pd.DataFrame(results)
df



plt.figure(figsize=(7,5))
plt.plot(df["Utilization (ρ)"], df["Avg Latency"], marker='o')
plt.xlabel("System Utilization (ρ)")
plt.ylabel("Average Latency")
plt.title("Latency vs Utilization in Distributed Service")
plt.grid(True)
plt.show()



plt.figure(figsize=(7,5))
plt.plot(df["Utilization (ρ)"], df["P95 Latency"], label="P95", marker='o')
plt.plot(df["Utilization (ρ)"], df["P99 Latency"], label="P99", marker='s')
plt.xlabel("System Utilization (ρ)")
plt.ylabel("Tail Latency")
plt.title("Tail Latency Explosion Near Saturation")
plt.legend()
plt.grid(True)
plt.show()



def simulate_two_servers(arrival_rate, service_rate, num_requests=10000):
    arrival_times = np.cumsum(np.random.exponential(1/arrival_rate, num_requests))

    server_free_time = [0,0]
    finish_times = []

    for arrival in arrival_times:
        next_server = np.argmin(server_free_time)
        start = max(arrival, server_free_time[next_server])
        service = np.random.exponential(1/service_rate)
        finish = start + service
        server_free_time[next_server] = finish
        finish_times.append(finish)

    latency = np.array(finish_times) - arrival_times
    return latency



lat1 = simulate_mm1(50,70)
lat2 = simulate_two_servers(50,70)

print("Single Server Avg Latency:", np.mean(lat1))
print("Two Server Avg Latency:", np.mean(lat2))



labels = ["Single Server", "Two Servers"]
values = [np.mean(lat1), np.mean(lat2)]

plt.figure(figsize=(6,4))
plt.bar(labels, values)
plt.ylabel("Average Latency")
plt.title("Impact of Horizontal Scaling")
plt.show()

